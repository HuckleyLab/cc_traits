{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "import seaborn as sns\n",
    "from pyearth import Earth\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AVOID_LONG_COMPUTATIONS = False\n",
    "SCORING = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:right\" src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\" width=60px)/>\n",
    "# Traits and Range Shifts: Parameter Optimization +  Feature Engineering\n",
    "\n",
    "## <small>work by [Tony Cannistra](http://www.github.com/acannistra) and the [Buckley Lab](http://faculty.washington.edu/lbuckley) at the University of Washington</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In experimentation with several regression techniques aiming to harness the predictive value of physiological traits, two techniques proved best: Multivariate Adaptive Regression Splines (MARS) and Support Vector regression (SVR). \n",
    "\n",
    "We hone the predictions of those two methods for this application by performing two optmizations: grid parameter search and feature engineering. \n",
    "\n",
    "\n",
    "### Contents\n",
    "1. [Grid Search Background](#Grid-Search-Background)\n",
    "1. [MARS Grid Search](#MARS)\n",
    "1. [SVR Parameter Grid Search](#)\n",
    "1. [Feature Engineering](#Feature-Engineering)\n",
    "\n",
    "## Grid Search Background\n",
    "\n",
    "Many algorithms have a large number of hyperparameters which can be tuned to improve performance. Often the pursuit of the optimal hyperparameters is arduous, especially when there are a large number of them (or a large hyperparameter space). \n",
    "\n",
    "Scikit-Learn provides two functions for hyperparameter search: [`model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and [`model_selection.RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV). `GridSearchCV` provides an exhaustive search over parameter values and uses cross-validation and a scoring function to evaluate the most optimal parameters from the space given. `RandomizedSearchCV` samples candidate parameter values from given distributions. \n",
    "\n",
    "We evaluate the resulting models by using a holdout dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['Bio1_mean_nosyn' 'Bio1_std_nosyn' 'Bio1_var_nosyn' 'Bio1_mean_inclsyn'\n",
      " 'Bio1_std_inclsyn' 'Bio1_var_inclsyn' 'oceanity_ks' 'oceanity_o'\n",
      " 'oceanity_os' 'oceanity_sks' 'oceanity_so' 'oceanity_sos'\n",
      " 'dispersal_mode_animal' 'dispersal_mode_gravity' 'dispersal_mode_water'\n",
      " 'dispersal_mode_wind' 'BreedSysCode_1.0' 'BreedSysCode_2.0'\n",
      " 'BreedSysCode_3.0' 'BreedSysCode_4.0' 'Grime_c' 'Grime_cs' 'Grime_csr'\n",
      " 'Grime_r' 'Grime_s' 'Grime_sr']\n",
      "Examples: 93\n"
     ]
    }
   ],
   "source": [
    "Data = {} ## master dictionary containing all data transformations\n",
    "\n",
    "responseVar = \"migration_m\"\n",
    "\n",
    "plants_master = pd.read_csv(\"../data/plants5.csv\")\n",
    "\n",
    "\n",
    "drop_features = [\"Taxon\",\n",
    "                 \"migr_sterr_m\", \n",
    "                 \"shift + 2SE\", \n",
    "                 'signif_shift',\n",
    "                 \"signif_shift2\",\n",
    "                 \"dispmode01\",\n",
    "                 \"DispModeEng\", ## what is this\n",
    "                 \"shift + 2SE\",\n",
    "                ]\n",
    "\n",
    "categorical_features = [\"oceanity\",\n",
    "                        \"dispersal_mode\",\n",
    "                        \"BreedSysCode\",\n",
    "                        \"Grime\"]\n",
    "\n",
    "##\n",
    "# We leave the data as-is, with missing values and categorical variables.\n",
    "##\n",
    "\n",
    "plants = pd.get_dummies(plants_master, columns=categorical_features)\n",
    "\n",
    "# drop features we don't want\n",
    "features = plants.drop(drop_features, axis=1)\n",
    "\n",
    "# drop features with n/a or NaN\n",
    "## axis = 1 drops columns with any NAs, axis = 0 drops rows with any NAs\n",
    "features.dropna(axis=1, inplace=True)\n",
    "\n",
    "# extract and remove target variable\n",
    "target   = features[responseVar]\n",
    "features.drop([responseVar], inplace=True, axis=1)\n",
    "\n",
    "X, x_test, Y, y_test = train_test_split(features, target, test_size=0.30)\n",
    "\n",
    "print(\"Features: \",X.columns.values)\n",
    "print(\"Examples:\", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARS\n",
    "[mars]: http://www.jstor.org/stable/2241837?origin=JSTOR-pdf&seq=1#page_scan_tab_contents \"Friedman 2001\"\n",
    "[leathwick]: http://www.web.stanford.edu/~hastie/Papers/Ecology/fwb_1448.pdf \"Leathwick et al. 2005\"\n",
    "The multivariate adaptive regression splines approach [(Friedman 2001)][mars] fits piece-wise linear basis functions in order to better approximate nonlinear realtionships. It has limited uses in ecology, with the first published example in [Leathwick et al. 2005][leathwick]. It offers our best bet for a bencmark that has any hope of capturing these nonlinear relationships.\n",
    "\n",
    "### Benchmarking \n",
    "Nothing fancy, how does the algorithm perform with base parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mars = Earth()\n",
    "\n",
    "## this is a long computation\n",
    "if not AVOID_LONG_COMPUTATIONS:\n",
    "    -cross_val_score(mars, features, target, cv=LeaveOneOut(), scoring=SCORING, n_jobs=-1).mean()\n",
    "else:\n",
    "    print(\"Long computations disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not AVOID_LONG_COMPUTATIONS:\n",
    "    -cross_val_score(mars, features, target, cv=KFold(2), scoring=SCORING, n_jobs=-1).mean()\n",
    "else:   \n",
    "    print(\"Long computations disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.5871283592\n"
     ]
    }
   ],
   "source": [
    "criteria = ('rss', 'gcv', 'nb_subsets')\n",
    "\n",
    "mars = Earth(max_degree=3, feature_importance_type=criteria)\n",
    "mars.fit(features, target)\n",
    "\n",
    "if not AVOID_LONG_COMPUTATIONS:\n",
    "    print(-cross_val_score(mars, features, target, cv=LeaveOneOut(), scoring=SCORING, n_jobs=-1).mean())\n",
    "else:\n",
    "    print(\"Long computations disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR\n",
    "\n",
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.706767847145617"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "base = SVR()\n",
    "errs = cross_val_score(base, scale(X), Y, cv=LeaveOneOut(), scoring=SCORING, n_jobs=-1)\n",
    "-errs.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good. Let's see if we can do better. \n",
    "\n",
    "### Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Parameters:\n",
      "  ['kernel', 'verbose', 'C', 'gamma', 'coef0', 'epsilon', 'cache_size', 'degree', 'tol', 'shrinking', 'max_iter']\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Parameters:\\n \", str(list(base.get_params().keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score=0,\n",
       "       estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': array([  1.00000e-02,   1.00000e-01,   1.00000e+00,   1.00000e+01,\n",
       "         1.00000e+02,   1.00000e+03,   1.00000e+04,   1.00000e+05,\n",
       "         1.00000e+06,   1.00000e+07,   1.00000e+08,   1.00000e+09,\n",
       "         1.00000e+10]), 'gamma': array([  1.00000e-09,   1.00000e-08,   1.00000e-07,   1.00000e-06,\n",
       "         1.00000e-05,   1.00000e-04,   1.00000e-03,   1.00000e-02,\n",
       "         1.00000e-01,   1.00000e+00,   1.00000e+01,   1.00000e+02,\n",
       "         1.00000e+03])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_grid = {\n",
    "    'C'     :  np.logspace(-2, 10, 13), \n",
    "    'gamma' : np.logspace(-9, 3, 13),\n",
    "}\n",
    "\n",
    "SVR_grid = GridSearchCV(base,\n",
    "                        param_grid = params_grid,\n",
    "                        scoring=SCORING,\n",
    "                        n_jobs = -1,\n",
    "                        error_score = 0,\n",
    "                        cv=KFold(5)) \n",
    "\n",
    "SVR_grid.fit(scale(X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10000000.0,\n",
      " 'cache_size': 200,\n",
      " 'coef0': 0.0,\n",
      " 'degree': 3,\n",
      " 'epsilon': 0.1,\n",
      " 'gamma': 1.0000000000000001e-09,\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n",
      "{'C': 1.0,\n",
      " 'cache_size': 200,\n",
      " 'coef0': 0.0,\n",
      " 'degree': 3,\n",
      " 'epsilon': 0.1,\n",
      " 'gamma': 'auto',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "best = SVR_grid.best_estimator_\n",
    "pp.pprint(best.get_params())\n",
    "pp.pprint(base.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base MSE on test: -21.3151848986\n",
      "Best MSE on test: -21.0046735253\n"
     ]
    }
   ],
   "source": [
    "print(\"Base MSE on test:\", cross_val_score(base, scale(x_test), y_test, scoring='neg_mean_squared_error').mean())\n",
    "\n",
    "print(\"Best MSE on test:\", cross_val_score(best, scale(x_test), y_test, scoring='neg_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[featEng]: http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/ \"Source\"\n",
    "\n",
    "## Feature Engineering\n",
    "> Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. [source] [featEng]\n",
    "\n",
    "This applies to our problem in several ways. The most straightforward work in feature engineering we've already done most of. Namely, the encoding of categorical features in a way that creates binary features for each category of each categorical feature. This is known as *one hot* encoding. \n",
    "\n",
    "\n",
    "A much more interesting approach to feature engineering is the creation of new features by linearly combining existing features in some way. \n",
    "\n",
    "The challenge there is that creating pairwise multiples of, say, 20 features leads us to move from 20 features to 400 features. Learning any model on these 400 features is sure to lead to overfitting, especially in a linear model. We can combat this with regularization. \n",
    "\n",
    "There are two interesting forms of regularization that we can use for this project: ridge regression and LASSO regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
